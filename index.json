[{"categories":null,"contents":"I wanted to work on projects that would benefit society and found Google Summer of Code (GSoC) to be the perfect opportunity. I worked with Free UK Genealogy, an organization that aims at the transcription of family data and explored Named Entity Recognition (NER) more deeply. I created an end-to-end framework that extracts text from wills, using OCR, and seeds them into a database with named entities. A big constraint in the project was a lack of adequate training samples to build an accurate NER model. To solve this, I used a bi-LSTM-CRF architecture that uses a small training set and unlabelled corpora. To train the NER system, I made use of SpaCy’s language model. For NER, the paper “Neural architectures for named entity recognition” formed the basis of my NER model at Google Summer of Code. The system developed through GSoC has enabled the organization to extract structured data from scanned documents and build a new database for its users. The hybrid tagging structure was very helpful for labeling overlapping entity sets present in the data. The application developed through GSoC has enabled the organization to retrieve structured data from wills from 18th century England and launch a new database. ","permalink":"https://shahsaumya.github.io/projects/contributions/probate_parsing/","tags":["named entity recognition","Tesseract OCR","Image Processing","SpaCy","Python"],"title":"Probate Parsing"},{"categories":null,"contents":"For my graduation project, I worked with Prof. Meliha Yetisgen as a graduate research assistant in the BioNLP lab and worked on a project that aimed at extracting family history from patients’ clinical narratives. The project required me to develop an end-to-end system from preparing the training dataset for the individual tasks to selecting the model, algorithm, and implementation.\nThis project can be divided into two main subtasks:\n• Entity identification (family members and disease names) - This involves getting the family members and disease names (unassociated). I decided to use Clinical BERT for Named Entity Recognition by fine-tuning BERT for Token Classification. This method required to use of the transformer and add a final decision layer for NER. This method gave very promising results and was able to identify most, if not all entities.\n• Family history extraction - the system is expected to associate family members and corresponding observations. The strategy for this subtask is to start small and build on top by analyzing the results. Most baseline relation extraction models use POS tags to denote a relation. Using the POS tags as a feature in the relation extraction task, I am using Clinical BERT for relation extraction as well. The implementation for this follows a ”matching the blanks” method. Since this is similar to masked language modeling, it would be a good way to use BERT for relation extraction.\nThe dataset provided for this task is from Patient Provided Information (PPI) questionnaires, which are usually stored in semi-structured/unstructured formats.\n","permalink":"https://shahsaumya.github.io/publications/bionlp/","tags":["natural language processing","biomedical information extraction","named entity recognition","relation extraction","pytorch","bert"],"title":"Family History Extraction using Clinical Narratives"},{"categories":null,"contents":"This paper delineates the automation of question generation as an extension to existing Visual Question Answering (VQA) systems. Through our research, we have been able to build a system that can generate questions and answer pairs on images. It consists of two separate modules - Visual Question Generation (VQG) which generates questions based on the image, and a Visual Question Answering (VQA) module that produces a befitting answer that the VQG module generates. Through our approach, we not only generate questions but evaluate the questions generated by using a question answering system. Moreover, with our methodology, we can generate question-answer pairs as well as improve the performance of VQA models. It eliminates the need for human intervention in dataset annotation and also finds applications in the field of the educational sector, where the requirement of human input for textual questions has been essential till now. Using our system, we aim to provide an interactive interface which helps the learning process among young children. Received a grant from the University of Mumbai\n","permalink":"https://shahsaumya.github.io/publications/automated-vqg/","tags":["natural language processing","computer vision","visual question answering","visual question generation","recurrent neural network","vqa dataset","image feature extraction","long short term memory","gated recurrent unit","convolution neural network","attention module","e-learning system"],"title":"Automated Question Generation and Answer Verification using Visual Data"},{"categories":null,"contents":"Water scarcity is hitting new peaks every day and is exacerbated by the current rapid climatic change. Demand for clean water in India is very high, especially for agriculture and consumption. One way to cater to these needs is through rainwater harvesting. Through this paper, we propose a framework that optimizes the site selection for reservoirs by intersecting various data points. Our framework uses a three-step approach to combine stream networks, digital elevation, and soil quality to produce the most viable reservoir sites. Our framework is easy to implement and highly scalable. For the purpose of this paper and a proof of concept, we restrict our focus to the arid Beed district in the state of Maharashtra, India. Our approach provides consistent results that are corroborated by the manual inferences that can be drawn from the data under consideration.\nWe create a simulation model that uses weather data and GIS data to formulate rainwater collection to optimize rainwater harvesting. My team developed a framework to optimize rainwater harvesting by finding viable reservoir locations using hydrological data, digital elevation maps and soil information using GIS software. Our project received a research grant from the University of Mumbai.\n","permalink":"https://shahsaumya.github.io/publications/gis/","tags":["Geoinformatics System","ArcGIS","ISRO Mosdac","SoilGrids","Bhuvan Topological Data","HydroSHEDS","Decision Support Systems"],"title":"Optimization of Rainwater Harvesting Sites using GIS"},{"categories":null,"contents":"To build the summarization, we followed four steps for end-to-end data extraction and summary generation - data preprocessing, content selection, information ordering and content realization.\n Data Preprocessing - The process involved collecting raw data, segmenting sentences, coverting to tokens, stop word removal, lemmatization and stemming Content Selection - We used Latent Dirichlet Allocation to select the most important sentences that contribute to the summary. To improve the accuracy of LDA, we used TF-IDF scores that rank the words of a document based on importance and relevance. Information Ordering - This phase helps in ordering these selected sentences so that the summary is coherent. We used cosine similarity to discard any redundant sentences and use pairwise cosine score to determine the most coherent ordering. Content Realization - This phase makes the final touches on the sentences, removing any extraneous parts of sentences that would make it wordy etc. To do this, we used methods like removing parenthesis, eliminating sentences shorter than 8 words, removing adverbs etc.  ","permalink":"https://shahsaumya.github.io/projects/creations/automated_summarization/","tags":["Topic Modelling","Summarization","Python","Latent Dirichlet Allocation","Gensim"],"title":"Automated Topic-Based Extractive Summarization System"},{"categories":null,"contents":"In this project, we contrasted two different methods for Named Entity Recognition on 3 benchmark datasets. The first method was the biLSTM-CRF neural arcitecture that has been the norm for Named Entity Recognition versus using language models like BERT or ELMo. We use the contextual word embeddings from ELMo and pretrain BERT for Named Entity Recognition. The datasets we used were the CoNLL 2003 dataset, MIT movie dataset and the GMB NER dataset. In our analysis, we find that language models do better than the neural architecture with ELMo outperforming both BERT and bi-LSTM-CRF.\n","permalink":"https://shahsaumya.github.io/projects/creations/ner-using-lms/","tags":["Language Models","NER","Python","BERT","ELMo","PyTorch","AllenNLP","CoNLL 2003"],"title":"Named Entity Recognition using BERT and ELMo"},{"categories":null,"contents":"In recent years, we have witnessed a significant progress in various fields of AI, such as computer vision, as well as language understanding. These progress motivated researchers to address a more challenging problem: question answering and question generation on visual data. This problem combines both image understanding as well as language understanding. Essentially, this task is defined as follows: an image along with a question about that image is the input to the AI system, and the intelligent system is supposed to output a correct answer to the question with respect the input image. However, taking a step beyond that, we through this project, aim to create a system which on taking an image as input generates a question-answer pair and when the user submits the answer, verification of the answer takes place. Our solution thus involves the usage of both the Visual Question Generation (VQG) and VQA model in a sequential manner. The VQG module generates reasonable questions given an image whereas, the VQA module generates natural answers given a question and an image. On the basis of the image, the VQG and VQA module will be used to get the correct question and then the answer.The proposed solution will aim towards bridging the gap and assimilating verification of answers on the existing question answer generation platforms. The most immediate application is as an automatic dataset annotation tool. We can reduce the need for human intervention for dataset annotation. With an increasing need for varied datasets for Deep Learning, such applications can help reduce the need for human labour. Further, the solution will benefit the educational sector, as well as help, improve existing authentication using captcha. In the educational sector, children can use such an application to learn to answer basic questions from images. While these same images can be used as an extra step in authentication to increase the security of the project. Received a grant from the University of Mumbai\n","permalink":"https://shahsaumya.github.io/projects/creations/automated_vqg/","tags":["Natural Language Processing","Computer Vision","Machine Learning","Tensorflow","Keras","Python"],"title":"Automated Question Generation and Answer Verification using Digital Data"},{"categories":null,"contents":"Made under the guidance of my college professor Prof. Lakshmi Kurup, the application strives to teach basic grammar, specifically comma punctuation to small children. Using a list of subjects, objects, and verbs, the web app can construct a sentence and check user input at each stage using Context Free Grammar. Hints are produced based on the input entered by the user, so as to guide the user better.\n","permalink":"https://shahsaumya.github.io/projects/creations/grammar_tutor/","tags":["Python","Django","HTML","CSS3","AJAX"],"title":"Grammar Tutor"},{"categories":null,"contents":"In a hackathon project in the field of fintech, my team and I developed a stock management system. It makes use of LSTMs to predict the effect of Twitter feed and Reuters News feed on stock prices based on their sentiment scores. Based on financial documents submitted by each company important metrics that identify dead assets, ROI etc. We also perform analysis of long financial documents in-order to extract the important topics discussed in the report with the help of NLP technique TF-IDF. Our financial portfolio management system was selected in the top 5 projects in the hackathon.\n","permalink":"https://shahsaumya.github.io/projects/creations/portfolio_mgmt/","tags":["Fintech","Deep Learning","Sentiment Analysis","TF-IDF","Pandas","Python"],"title":"Portfolio Management System"},{"categories":null,"contents":"Built a Gamma-Poisson classifier to represent the similarity between records in disparate datasets. Our efforts showed impressive results, even better than the probabilistic approach proposed by Fellegi-Sunter. The prototype has now been launched as a stand-alone framework for entity resolution developed by the company, and I have also managed to submit a paper based on our research to AIStats. This classifier can work with streaming, sparse and sequential data\n","permalink":"https://shahsaumya.github.io/projects/creations/gammalink/","tags":["Probabilistic Graphical Models","Pandas","Record Linkage","matplotlib","Python"],"title":"Gammalink"},{"categories":null,"contents":"A project that provides information on crop sustenance based on geo-location, rainfall. land-surface temperature and soil nutrient analysis. It is a one-stop shop for farmers to learn about air, water soil conditions in and around their farm, receive alerts about any possible disasters/weather alerts and get recommendations about crop cultivation. The data required for this forecasting system would be taken from varied sources such as ISRO MOSDAC, ISRO Vedas, each having different characteristics. The disaster update mechanism was facilitated by ISRO MOSDAC.It bagged the 2nd position at State level Hackathon\n","permalink":"https://shahsaumya.github.io/projects/creations/smart_crop/","tags":["Machine Learning","Scikit-learn","Time Series Analysis","Python","Django","Pandas","Google Translate API"],"title":"Smart Crop - A Farming Management System"},{"categories":null,"contents":"In this hackathon, sponsored by the Indian Space Research Organization, we built a recommendation system for farmers to provide actionable advice on the crops that would be ideal based on the location, rainfall and soil time-series data. The data required for this forecasting system would be taken from varied sources such as ISRO MOSDAC, ISRO Vedas, each having different characteristics. Designing a system that would take into account the various parameters gave an insight into topics like multivariate time-series systems. The prototype presented the 4th prize at the competition, which motivated us to apply for a grant to the Department of Science and Technology.\n","permalink":"https://shahsaumya.github.io/projects/creations/crop_recommendation_sys/","tags":["Machine Learning","Scikit-learn","Time Series Analysis","Python","Django","Google Translate API"],"title":"Crop Recommendation System"},{"categories":null,"contents":"As a part of my Web Technologies Lab, I built a movie recommender system using collaborative filtering (CF). From past user activity (like/dislike or ratings), I built a rule-based model, that uses the neighborhood-based CF. This also allows a prediction from like-minded users with similar ratings. The MovieLens dataset was used and employed user interactions like ratings to drive the GRU.\n","permalink":"https://shahsaumya.github.io/projects/creations/movie_recommendation/","tags":["Collaborative Filtering","Python","Django","HTML","CSS3","AJAX"],"title":"Movie Recommendation System"},{"categories":null,"contents":"Made in the Smart India Hackathon for the Indian Space Research Organisation, my team built a web app that recorded keystrokes of a user to create a dataset capturing his/her typing style. The project used a C-RNN-GAN which would distinguish between a legitimate user and a malicious user based on his/her typing style based on trust metrics. The training parameters for each user will be unique, and thus a trust mechanism would reward or penalize based on correct or incorrect typing behavior, based on the RNN network.\nSelected in the top 4 projects in Smart India Hackathon, a government-organized, national-level hackathon.\n","permalink":"https://shahsaumya.github.io/projects/creations/continuous_user/","tags":["Deep Learning","Generative Adversarial Network","Keystroke Dynamics","Python","Django"],"title":"Continuous User Identification System"},{"categories":null,"contents":"A forum web app for the entire college as a member of the Technical Co-Committee in DJ Literary Society. The forum made use of different categories for the different departments and threads in each of these categories. It is built using the Misago framework built on top of Django.\n","permalink":"https://shahsaumya.github.io/projects/creations/college_forum/","tags":["Python","Django","POSTgreSQL","HTML","CSS3","AJAX"],"title":"College Online Forum"},{"categories":null,"contents":"","permalink":"https://shahsaumya.github.io/projects/creations/online_archive/","tags":null,"title":""},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml\n[outputs] home = [\u0026quot;HTML\u0026quot;, \u0026quot;JSON\u0026quot;] Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category\n... \u0026quot;contents\u0026quot;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026quot;tags\u0026quot;:{{ .Params.tags | jsonify }}{{end}}, \u0026quot;categories\u0026quot; : {{ .Params.categories | jsonify }}, ... Edit fuse.js options to Search static/js/search.js\nkeys: [ \u0026quot;title\u0026quot;, \u0026quot;contents\u0026quot;, \u0026quot;tags\u0026quot;, \u0026quot;categories\u0026quot; ] ","permalink":"https://shahsaumya.github.io/search/","tags":null,"title":"Search Results"}]